# 线性回归API
* sklearn.liner_model.LinerRegression(fit_intercept=True)
    * 通过正规方程进行计算
    * fit_intercept：是否计算偏置
    * LinerRegression.coef_: 回归系数
    * LinerRegression.intercept_: 偏置
* sklearn.liner_model.SGDRegression(loss="squared_loss", fit_intercept=True,learning_rate="invscaling", eta0=0.01)
    * SGDRegression类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。
    * loss: 损失类型
        * loss = "squared_loss": 普通最小二乘法
    * fit_intercept: 是否计算偏置
    * learning_rate: string, optional
        * 学习率填充
        * 'constant': eta = eta0
        * 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]
        * 'invscaling': eta = eta0 / pow(t, power_t)
            * power_t=0.25:存在父类当中
        * 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。
    * SGDRegressor.coef_：回归系数
    * SGDRegressor.intercept_：偏置
    
> sklearn提供给我们两种实现的API， 可以根据选择使用

# 回归性能评估
均方误差((Mean Squared Error)MSE)评价机制
![image](https://dakastatic.oss-cn-beijing.aliyuncs.com/hanqun/%E6%88%AA%E5%B1%8F2021-01-25%20%E4%B8%8B%E5%8D%881.50.20.png)
* sklearn.metrics.mean_squared_error(y_true, y_pred)
    * 均方误差回归损失
    * y_true: 真实值
    * y_pred: 预测值
    * return: 浮点数结果

# 带有L2正则化的线性回归---岭回归
岭回归其实也是一种线性回归，只不过在算法建立回归方程的时候，加上正则化的限制，从而达到解决过拟合的效果。

岭回归API：
* sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)
    * 具有l2正则化的线性回归
    * alpha:正则化力度，也叫 λ
        * λ取值：0~1 1~10
    * solver:会根据数据自动选择优化方法
        * sag:如果数据集、特征都比较大，选择该随机梯度下降优化
    * normalize:数据是否进行标准化
        * normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据
    * Ridge.coef_:回归权重
    * Ridge.intercept_:回归偏置

Ridge方法相当于SGDRegressor(penalty='l2', loss="squared_loss"),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)

* sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)
    * 具有l2正则化的线性回归，可以进行交叉验证
    * coef_:回归系数