# 特征工程
## 一、什么是特征工程
特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的工程。
意义： 会直接影响机器学习的效果。
## 二、特征工程包含内容
* 特征抽取/特征提取
* 特征预处理
* 特征降维
## 三、特征抽取/特征提取
机器学习算法 <---> 统计方法 <---> 数学公式 <br/>
文本类型 ---> 数值
### 1. 将任意数据(文本或图像)转换为可用于机器学习的数字特征。
> ⚠️特征值化是为了计算机更好地去理解数据
* 字典特征提取(特征离散化)
* 文本特征提取
* 图像特征提取(深度学习时常用)
### 2. 特征提取API
```python
import sklearn
sklearn.feature_extrction
```
### 3. 字典特征提取
* sklearn.feature_extrction.DictVectorizer(sparse=True, ...)
    * DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值，返回：sparse矩阵
    * DictVectorizer.inverse_transeform(X) X:array数组或者sparse矩阵，返回：转换之前数据格式
    * DictVectorizer.get_feature_names() 返回类别名称

sparse: 是否稀疏矩阵 目的：节省内存，提高加载效率

在矩阵中，若数值为0的元素数目远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵；与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵。定义非零元素的总数比上矩阵所有元素的总数为矩阵的稠密度。

**sklearn中sparse矩阵对象中toarray()方法可以把矩阵转换为二维数组。**

字典特征提取时，对于类别特征，按one-hot编码处理。

应用场景：

* 1.数据集中类别特征较多时
    * 1)数据集特征->字典类型
    * 2)DictVectorizer转换
* 2.本身拿到的数据就是字典类型
### 4. 文本特征抽取
单词作为特征

特征：特征词

作用：对文本数据进行特征值化



#### (1)CountVectorizer词频提取器

* sklearn.feature_extraction.CountVectorizer(stop_words=[], ...)
    * 返回词频矩阵
    * CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象，返回：sparse矩阵
    * CountVectorizer.inverse_transeform(X) X:array数组或者sparse矩阵，返回：转换之前数据格式
    * CountVectorizer.get_feature_names() 返回类别名称
    * 统计每个样本特征词出现的个数
    * 当使用中文时，不支持中文分词，且不支持单个中文字
    * stop_words：停用词

#### (2)TfidfVectorizer提取器
关键词： 在某一个类别的文章中，出现的次数很多，但是在其他文章中出现的次数很少。

TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他的文章中很少出现，则认为此词或者短语
具有很好的类别区分能力，适合用来分类。

TF-IDF的作用：用以评估一字词对于一个文件集或语料库中的其中一份文件的重要程度
* 公式：
    * 词频（term frequency，tf）指的是某一个给定的词语在文件中出现的频率。
    * 逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一个特定词语的idf，可以由总文件的
    数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到。
    
最终得到的结果可以理解为重要程度。
> 注：假如一篇文章的总词汇数是100个，而词语非常出现了5次，那么"非常"一词在该文件中的词
* sklearn.feature_extraction.TfidfVectorizer(stop_words=[], ...)
    * 返回词的权重矩阵
        * TfidfVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象，返回：sparse矩阵
        * TfidfVectorizer.inverse_transeform(X) X:array数组或者sparse矩阵，返回：转换之前数据格式
        * TfidfVectorizer.get_feature_names() 返回单词列表
        * 当使用中文时，不支持中文分词，且不支持单个中文字
        * stop_words：停用词 

## 四、特征预处理
### 1. 什么是特征预处理
> scikit-learn的解释：<br/>
> 通过一些转换函数将特征数据转换成更加适合算法模型的特征数据的过程。
### 2. 包含内容
* 数值型数据的无量纲化
    * 归一化
    * 标准化
### 3. 特征预处理API
```python
import sklearn
sklearn.preprocessing
```

### 4. 为什么要进行归一化/标准化
* 特征的单位或大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响(支配)
目标结果，使得一些算法无法学习到其他的特征。

### 5. 归一化
#### (1)定义
通过对原始数据进行变换把数据映射到(默认为[0,1])之间。
#### (2)公式
X' = (x-min)/(max-min)
X'' = X'*(mx - mi) + mi
> 作用于每一列，max为一列的最大值，min为一列的最小值，那么X''为最终结果，mx，mi分别为指定区间值默认mx为1，mi为0
#### (3)API
* sklearn.preprocessing.MinMaxScaler(feature_range(0, 1), ...)
    * MinMaxScaler.fit_transform(X) 
        * X:numpy array格式的数据
    * 返回值： 转换后的形状相同的array

问题： 如果数据中异常点较多，会有什么影响。
#### (4)归一化总结
注意最大值和最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。
### 6. 标准化
#### (1)定义
通过对原始数据进行变换，把数据变换到均值为0，标准差为1范围内。

#### (2)公式
X' = (x-mean)/σ
> 作用于每一列，mean为平均值，σ为标准差
* 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
* 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小
#### (3)API
* sklearn.preprocessing.StandardScalar()
    * 处理之后，对每列来说，所有数据都聚集在均值0附近，标准差为1
    * MinMaxScaler.fit_transform(X) 
        * X:numpy array格式的数据
    * 返回值： 转换后的形状相同的array
#### (4)标准化总结
在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。
## 四、特征降维
### 1. 降维
降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组"不相关"主变量的过程。
> 正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大
### 2. 降维的两种方式
* 特征选择
* 主成分分析(可以理解成一种特征提取的方式)
### 3. 特征选择
#### (1) 什么是特征选择
数据中包含冗余或相关变量(或称特征、属性、指标等)，旨在从原有特征中找出主要特征。
#### (2) 方法
* Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联
    * 方差选择法：低方差特征过滤
    * 相关系数：特征与特征之间的相关程度
* Embeded(嵌入式)：算法自动选择特征(特征与目标值之间的关联)
    * 决策树：信息熵、信息增益
    * 正则化：L1、L2
    * 深度学习： 卷积等
> 对于embeded，在讲算法时再进行介绍
#### (3) 模块
```python
import sklearn
sklearn.feature_selection
```
### 4. 过滤式
#### (1) 低方差特征过滤
删除低方差的一些特征，前面讲过方差的意义。再结合方差的大小来考虑这个方式的角度。
* 特征方差小：某个特征大多样本的值比较相近。
* 特征方差大：某个特征很多样本的值都有差别。
##### (a) API
* sklearn.feature_selection.VarianceThreshold(threshold=0.0, ...)
    * 删除所有低方差特征
    * VarianceThreshold.fit_transform(X)
        * X: numpy array格式的数据
        * 返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。
#### (2) 相关系数
* 皮尔逊相关系数：
    * 反映变量之间相关关系密切程度的统计指标
* 公式
![image](https://dakastatic.oss-cn-beijing.aliyuncs.com/hanqun/%E6%88%AA%E5%B1%8F2021-01-24%20%E4%B8%8A%E5%8D%889.54.51.png)
* 特点：相关系数值介于-1与+1之间，即-1<=r<=1,其性质如下：
    * 当r>0时，表示两个变量正相关，r<0时，两个变量为负相关。
    * 当|r|=1时，表示两变量为完全相关，当r=0时，表示两变量间无相关关系。
    * 当0<|r|<1时，表示两变量存在一定程度的相关，且|r|越接近1，两变量间线性关系越密切；|r|越接近0，表示两变量的线性相关越弱。
    * 一般可按三等级划分：|r|<0.4为低度相关，0.4<=|r|<0.7为显著性相关，0.7<=|r|<1为高度线性相关。

* API:  from scipy.stats import pearsonr
    * x:(N,) array like
    * y:(N,) array like
    * returns: p-value
* 特征与特征之间相关性很高：
    * 选取其中一个特征
    * 加权求和
    * 主成分分析
* 